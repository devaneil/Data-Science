# Importing required libraries

import pandas as pd
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.utils import shuffle
import math

# Task 2 : Data Exploration

# Importing the red wine dataset 
wine_p = 'C:/Users/Devaneil Singh/Downloads/winequality-red.csv'
wine = pd.read_csv(wine_p, sep = ";", header = 0)

# Summary Statistics

# Printing the data types of all the columns
print('\n The data types of all the columns in the dataset : \n')
print(wine.dtypes)

# Printing the summary statistics of all the columns in the dataset
print('\n Summary statistics of all the columns in the dataset : \n')
print(wine.describe())

# Plotting a scatter matrix
scatter_matrix(wine[['fixed acidity', 'volatile acidity', 'citric acid','residual sugar', 'chlorides', 'free sulfur dioxide',
             'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']],
               alpha = 0.2, figsize = (16,16), diagonal = 'hist')
plt.show()

# Exploring quality vs sulphates
sns.barplot(x = 'quality', y = 'sulphates',data = wine)
plt.show()

# Exploring quality vs volatile acidity
sns.barplot(x = 'quality', y = 'volatile acidity', data = wine)
plt.show()

# Exploring quality vs alcohol
sns.barplot(x = 'quality', y = 'alcohol', data = wine)
plt.show()

# Data processing and preparation

# Printing the unique values in the quality column
print('\n The unique values in the quality column : \n')
print(wine['quality'].unique())

# Printing all the columns in the dataset
print('\n The columns in the red wine data : \n')
print(wine.columns)

# Defining an empty list 'rating' and inserting values low and high rating of wine respectively
rating = []
for i in range (len(wine['quality'])) :    
    if wine.loc[i,'quality'] >= 6 :
        rating.append("High")
    else :
        rating.append("Low")
        
# Creating rating column which has the values of the list rating
wine['rating'] = rating

# Printing all the values in the rating column along with their frequencies
print('\n The values in the rating column : \n')
print(wine['rating'].value_counts())

# Splitting data into training and testing dataset 
data = wine[['fixed acidity', 'volatile acidity', 'citric acid','residual sugar', 'chlorides', 'free sulfur dioxide',
             'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]
target = wine['rating']

# Task 3 : Data Modelling 

# Splitting the data into training and testing data

# Splitting the data into 50% training and 50% testing
X_train1, X_test1, y_train1, y_test1 = train_test_split(data, target, test_size = 0.5)

# Splitting the data into 60% training and 40% testing
X_train2, X_test2, y_train2, y_test2 = train_test_split(data, target, test_size = 0.4)

# Splitting the data into 80% training and 20% testing
X_train3, X_test3, y_train3, y_test3 = train_test_split(data, target, test_size = 0.2)

# kNN Classifier

print ("\n kNN Classifier \n")

# kNN on 50% training and 50% testing data 
print('\n kNN on 50% training and 50% testing : \n')

# Choosing the value of k
k1 = math.sqrt(len(X_train1))/2
print k1
# From the value of k we decided to put k = 13 to avoid ties
kClf1 = KNeighborsClassifier(13, weights = 'distance', p = 1)

fit1 = kClf1.fit(X_train1,y_train1)

y_pre1 = fit1.predict(X_test1)

print(" ")
print(y_pre1)

cm1 = confusion_matrix(y_test1, y_pre1)
print ("\n Confusion Matrix : \n")
print(cm1)
print(" ")

print classification_report(y_test1,y_pre1)

# Feature Selection
new_Ind = []
cur_MaxScore = 0.0
col_num = 11
col_Ind_Random = shuffle(range(0,col_num),random_state = 0)
for cur_f in range (0, col_num) :
    new_Ind.append(col_Ind_Random[cur_f])
    newData = data.iloc[:,new_Ind]
cur_Score1 = kClf1.score(X_test1, y_test1)
if cur_Score1 < cur_MaxScore:
    new_Ind.remove(col_Ind_Random[cur_f])
else:
    cur_MaxScore = cur_Score1    
print "Score with " + str(len(new_Ind)) + " selected features: " + str(cur_Score1)
print "There are " + str(len(new_Ind)) + " features selected:"
print new_Ind

# kNN on 60% training and 40% testing data
# Choosing the value of k
k2 = math.sqrt(len(X_train2))/2
print k2
print('\n kNN on 60% training and 40% testing : \n')
kClf2 = KNeighborsClassifier(15, weights = 'distance', p = 1)

fit2 = kClf2.fit(X_train2,y_train2)

y_pre2 = fit2.predict(X_test2)
print(" ")
print(y_pre2)

cm2 = confusion_matrix(y_test2, y_pre2)
print ("\n Confusion Matrix of 60% training and 40% testing data : \n")
print(cm2)
print(" ")

print classification_report(y_test2,y_pre2)

# Feature Selection
new_Ind = []
cur_MaxScore = 0.0
col_num = 11
col_Ind_Random = shuffle(range(0,col_num),random_state = 0)
for cur_f in range (0, col_num) :
    new_Ind.append(col_Ind_Random[cur_f])
    newData = data.iloc[:,new_Ind]
cur_Score2 = kClf2.score(X_test2, y_test2)
if cur_Score2 < cur_MaxScore:
    new_Ind.remove(col_Ind_Random[cur_f])
else:
    cur_MaxScore = cur_Score2    
print "Score with " + str(len(new_Ind)) + " selected features: " + str(cur_Score2)
print "There are " + str(len(new_Ind)) + " features selected:"
print new_Ind

# kNN 80% training and 20% testing data
# Choosing the value of k
k3 = math.sqrt(len(X_train3))/2
print k3
print('\n kNN on 80% training and 20% testing : \n')

kClf3 = KNeighborsClassifier(17, weights = 'distance', p = 1)

fit3 = kClf3.fit(X_train3,y_train3)

y_pre3 = fit3.predict(X_test3)

print(" ")
print(y_pre3)

cm3 = confusion_matrix(y_test3, y_pre3)
print ("\n Confusion Matrix of 80% training and 20% testing data : \n ")
print(cm3)
print(" ")

print classification_report(y_test3,y_pre3)

# Feature Selection
new_Ind = []
cur_MaxScore = 0.0
col_num = 11
col_Ind_Random = shuffle(range(0,col_num),random_state = 0)
for cur_f in range (0, col_num) :
    new_Ind.append(col_Ind_Random[cur_f])
    newData = data.iloc[:,new_Ind]
cur_Score3 = kClf3.score(X_test3, y_test3)
if cur_Score3 < cur_MaxScore:
    new_Ind.remove(col_Ind_Random[cur_f])
else:
    cur_MaxScore = cur_Score3    
print "Score with " + str(len(new_Ind)) + " selected features: " + str(cur_Score3)
print "There are " + str(len(new_Ind)) + " features selected:"
print new_Ind

# Decision Tree Classifier

print ("\n Decision Tree Classifier \n")

# DecisionTree on 50% training and 50% testing data 
print('\n Decision Tree on 50% training and 50% testing : \n')
DtClf1 = DecisionTreeClassifier()

fit4 = DtClf1.fit(X_train1,y_train1)

y_pre4 = fit4.predict(X_test1)

cm4 = confusion_matrix(y_test1,y_pre1)
print ("\n Confusion Matrix of 50% training and 50% testing data : \n")
print (cm4)

print classification_report(y_test1,y_pre4)

cur_Score4 = DtClf1.score(X_test1, y_test1)
if cur_Score4 < cur_MaxScore:
    new_Ind.remove(col_Ind_Random[cur_f])
else:
    cur_MaxScore = cur_Score4    
print "Score with " + str(len(new_Ind)) + " selected features: " + str(cur_Score4)
print "There are " + str(len(new_Ind)) + " features selected:"
print new_Ind

# DecisionTree on 60% training and 40% testing data 
print('\n Decision Tree on 60% training and 40% testing : \n')
DtClf2 = DecisionTreeClassifier(criterion = "entropy")

fit5 = DtClf2.fit(X_train2,y_train2)

y_pre5 = fit5.predict(X_test2)

cm5 = confusion_matrix(y_test2,y_pre5)
print ("\n Confusion Matrix of 60% training and 40% testing data : \n")
print (cm5)

print classification_report(y_test2,y_pre5)

# Feature Selection
new_Ind = []
cur_MaxScore = 0.0
col_num = 11
col_Ind_Random = shuffle(range(0,col_num),random_state = 0)
for cur_f in range (0, col_num) :
    new_Ind.append(col_Ind_Random[cur_f])
    newData = data.iloc[:,new_Ind]
cur_Score5 = DtClf2.score(X_test2, y_test2)
if cur_Score5 < cur_MaxScore:
    new_Ind.remove(col_Ind_Random[cur_f])
else:
    cur_MaxScore = cur_Score5    
print "Score with " + str(len(new_Ind)) + " selected features: " + str(cur_Score5)
print "There are " + str(len(new_Ind)) + " features selected:"
print new_Ind

# DecisionTree on 80% training and 20% testing data 
print('\n Decision Tree on 80% training and 20% testing : \n')
DtClf3 = DecisionTreeClassifier(criterion = "entropy", max_features = 7)

fit6 = DtClf3.fit(X_train3,y_train3)

y_pre6 = fit6.predict(X_test3)

cm6 = confusion_matrix(y_test3,y_pre6)
print ("\n Confusion Matrix of 80% training and 20% testing data : \n")
print (cm6)

print classification_report(y_test3,y_pre6)

# Feature Selection
new_Ind = []
cur_MaxScore = 0.0
col_num = 11
col_Ind_Random = shuffle(range(0,col_num),random_state = 0)
for cur_f in range (0, col_num) :
    new_Ind.append(col_Ind_Random[cur_f])
    newData = data.iloc[:,new_Ind]
cur_Score6 = DtClf3.score(X_test3, y_test3)
if cur_Score6 < cur_MaxScore:
    new_Ind.remove(col_Ind_Random[cur_f])
else:
    cur_MaxScore = cur_Score6
print "Score with " + str(len(new_Ind)) + " selected features: " + str(cur_Score6)
print "There are " + str(len(new_Ind)) + " features selected:"
print new_Ind
